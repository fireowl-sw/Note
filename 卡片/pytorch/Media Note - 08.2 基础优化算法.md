---
media: https://www.bilibili.com/video/BV1PX4y1g7KC?p=2
---
DECK: pytorch::08.2 基础优化算法
#### 梯度下降(最常见优化算法)
用途：当一个模型没有显示解的时候
- ![基础优化算法 - 00:21|50](基础优化算法PT21.103S.webp) [00:21](https://www.bilibili.com/video/BV1PX4y1g7KC?p=2&t=21.103206#t=21.10) 
1.随便挑选一个初始值$\mathbf{w}_{0}$ 接下来的时刻不断更新$\mathbf{w}_{0}$的位置使得它接近我们的最优解
2.$\mathbf{w}_{t}=\mathbf{w}_{t - 1}-\eta\frac{\partial\ell}{\partial\mathbf{w}_{t - 1}}$是更新法则。当前时刻=上一个时刻-标量(学习率)x(损失函数关于$\mathbf{w}_{t - 1}$的梯度)
3.看图直观的解释：$\mathbf{w}_{0}$(梯度指函数值增加最快的方向，负梯度($-\eta\frac{\partial\ell}{\partial\mathbf{w}_{t - 1}}$)则是值下降最快的地方)；学习率x(损失函数关于$\mathbf{w}_{t - 1}$的梯度)表示我沿着这个方向走多远。和$\mathbf{w}_{t - 1}$相加就代表得带后的$\mathbf{w}$
4.比喻：每次爬山沿着最陡的方向走一直到山顶
5.$\eta$ 超参数就是人要设置的数
<!--ID: 1742978888438-->

#### 学习率 $\eta$ 过大或则过小会如何
- ![基础优化算法 - 03:09|50](基础优化算法PT3M9.24S.webp) [03:09](https://www.bilibili.com/video/BV1PX4y1g7KC?p=2&t=189.240155#t=03:09.24) 
1. $\eta$ 太小：到达一个点需要走非常多的步骤。而计算梯度是深度学习里面最贵的部分。自动求导里提到每次计算梯度都需要进行存储，需要非常大的内存。所以需要尽可能少的计算梯度。(多算几次loss才会小)
2. $\eta$ 太大：由于走得太远，使得计算产生震荡路径（求导时可能除0，loss会是nan）
<!--ID: 1743044692408-->

#### 小批量随机梯度下降(最常见梯度下降版本)
- ![基础优化算法 - 04:12|50](基础优化算法PT4M12.938S.webp) [04:12](https://www.bilibili.com/video/BV1PX4y1g7KC?p=2&t=252.937531#t=04:12.94) 
1. 使用整个训练集算梯度，需要对整个损失函数求导。损失函数是整个样本平均求和开根号。很贵！时间久。
2. 小批量随机：随机采样b个样本来近似损失
3. $\frac{1}{b} \sum_{i \in I_b} \ell(\mathbf{x}_i, y_i, \mathbf{w})$ b很大的时候很精确，很小的时候没那么准确。但是计算复杂度(与样本个数线性相关)下降
4. b批量大小超参数
- ![基础优化算法 - 05:41|50](基础优化算法PT5M42S.webp) [05:41](https://www.bilibili.com/video/BV1PX4y1g7KC?p=2&t=341.999999#t=05:42.00)
1. 太小：gpu上不并行浪费计算资源
2. 太大：内存占用太大
<!--ID: 1743045294766-->



- ![基础优化算法 - 07:27|50](基础优化算法PT7M27.053S.webp) [07:27](https://www.bilibili.com/video/BV1PX4y1g7KC?p=2&t=447.05259#t=07:27.05) 